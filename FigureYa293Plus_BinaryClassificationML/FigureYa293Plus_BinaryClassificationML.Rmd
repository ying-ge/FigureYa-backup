---
title: "FigureYa293Plus_BinaryClassificationML"
params:
  author: "Zread AI Assistant"  
  reviewer: "Ying Ge"
output: html_document
---

**Author(s)**: `r params$author`  
**Reviewer(s)**: `r params$reviewer`  
**Date**: `r Sys.Date()` 

# Academic Citation
If you use this code in your work or research, we kindly request that you cite our publication:

Xiaofan Lu, et al. (2025). FigureYa: A Standardized Visualization Framework for Enhancing Biomedical Data Interpretation and Research Efficiency. iMetaMed. https://doi.org/10.1002/imm3.70005

If you use circlize in published research, please cite:

Gu, Z. circlize implements and enhances circular visualization in R. Bioinformatics 2014.

If you use ComplexHeatmap in published research, please cite:

Zuguang Gu, et al., Complex heatmaps reveal patterns and correlations in multidimensional genomic data, Bioinformatics, 2016.

Zuguang Gu. Complex Heatmap Visualization, iMeta, 2022.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 需求描述
# Demand description

基于机器学习的综合二分类预测模型构建，通过整合10种分类算法和100+种算法组合，筛选最优分类模型。

Comprehensive binary classification prediction model construction based on machine learning, integrating 10 classification algorithms and 100+ algorithm combinations to screen for optimal classification models.

# 应用场景
# Application scenarios

适用于各种二分类问题的机器学习建模：
- 疾病诊断预测（阳性 vs 阴性）
- 治疗响应预测（响应 vs 非响应）
- 预后分组（好 vs 差）
- 药物敏感性（敏感 vs 耐药）
- 分子亚型分类
- 生物标志物筛选

This framework is suitable for various binary classification machine learning modeling:
- Disease diagnosis prediction (positive vs negative)
- Treatment response prediction (responder vs non-responder)
- Prognosis grouping (good vs poor)  
- Drug sensitivity (sensitive vs resistant)
- Molecular subtype classification
- Biomarker screening

# 环境设置
# Environment Setup

```{r}
source("install_dependencies.R")

# 核心分类算法包
library(randomForest)
library(glmnet)
library(e1071)  # SVM
library(xgboost)
library(gbm)
library(nnet)  # neural network
library(MASS)  # LDA/QDA
library(class) # KNN
library(naivebayes)
library(kernlab) # kernelized SVM
library(party)  # Random Forest alternative
library(C50)    # Decision Tree
library(neuralnet)

# 特征选择包
library(Boruta)
library(FSelector)
library(mlr3)
library(mlr3learners)
library(mlr3extralearners)

# 评估和可视化
library(caret)
library(pROC)
library(dplyr)
library(tibble)
library(ggplot2)
library(ggsci)
library(tidyr)
library(ComplexHeatmap)
library(circlize)
library(RColorBrewer)
library(plotly)
library(VIM)
library(corrplot)

# 并行计算
library(parallel)
library(doParallel)

# 显示英文报错信息
Sys.setenv(LANGUAGE = "en") 
# 禁止chr转成factor
options(stringsAsFactors = FALSE) 

# 设置随机种子
seed <- 123
set.seed(seed)

# 设置并行核数
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)
```

# 输入文件
# Input Files

数据格式要求：
- 行为样本，第一列为样本名
- 第二列为分类标签（0/1 或 Negative/Positive）
- 第三列及以后为特征变量（基因表达、临床指标等）
- 支持多个验证数据集

Data format requirements:
- Rows represent samples, first column is sample names
- Second column is classification labels (0/1 or Negative/Positive)  
- Third column onwards are feature variables (gene expression, clinical indicators, etc.)
- Support multiple validation datasets

```{r}
# 这里用FigureYa293machineLearning的输入文件，用其中的生存状态OS作为二分类变量
train_data <- read.table("TCGA.txt", header = T,sep = "\t", quote = "", check.names = F)
test_data1 <- read.table("GSE57303.txt", header = T, sep = "\t", quote = "", check.names = F)
test_data2 <- read.table("GSE62254.txt.gz", header = TRUE, sep = "\t", quote = "", check.names = FALSE)

# 生成包含数据集的列表，删掉第三列生存时间，保留第二列作为二分类标签
mm <- list(train = test_data1[, -3], 
           test1 = test_data1[, -3], 
           test2 = test_data2[, -3])

# 数据预处理
mm <- lapply(mm, function(x){
  # 标准化数值特征
  numeric_cols <- sapply(x[,-c(1:2)], is.numeric)
  x[,-c(1:2)][numeric_cols] <- scale(x[,-c(1:2)][numeric_cols])
  
  # 处理缺失值
  x[,-c(1:2)] <- apply(x[,-c(1:2)], 2, function(col) {
    if(sum(is.na(col)) > 0) {
      col[is.na(col)] <- median(col, na.rm = TRUE)
    }
    return(col)
  })
  
  return(x)
})

# 统一标签格式
mm <- lapply(mm, function(x) {
  # 将标签统一转换为0/1格式
  if(is.character(x[,2]) || is.factor(x[,2])) {
    unique_labels <- unique(x[,2])
    if(length(unique_labels) == 2) {
      x[,2] <- ifelse(x[,2] == unique_labels[1], 0, 1)
    }
  }
  colnames(x)[2] <- "label"
  return(x)
})

# 检查数据质量
cat("=== 数据质量检查 ===\n")
for(i in names(mm)) {
  cat("Dataset:", i, "\n")
  cat("  Samples:", nrow(mm[[i]]), "\n")
  cat("  Features:", ncol(mm[[i]])-2, "\n")
  cat("  Label distribution:", table(mm[[i]]$label), "\n")
  cat("  Missing values:", sum(is.na(mm[[i]])), "\n\n")
}

# 准备建模数据
est_data <- mm$train
val_data_list <- mm[-1]

# 检查特征名称一致性
common_features <- Reduce(intersect, lapply(mm, function(x) colnames(x)[-c(1:2)]))
cat("Common features across all datasets:", length(common_features), "\n")

# 确保所有数据集使用相同的特征
mm <- lapply(mm, function(x) {
  x[, c(colnames(x)[1:2], common_features)]
})

est_data <- mm$train
val_data_list <- mm[-1]
```

# 特征选择工具函数
# Feature Selection Utility Functions

```{r}
# 基于重要性的特征选择
importance_feature_selection <- function(data, method = "rf", top_n = NULL) {
  features <- colnames(data)[-c(1:2)]
  
  if(method == "rf") {
    # 随机森林变量重要性
    rf_model <- randomForest(as.factor(label) ~ ., 
                            data = data[,-1], 
                            importance = TRUE, 
                            ntree = 500)
    importance_scores <- importance(rf_model)[,3]  # MeanDecreaseGini
    
  } else if(method == "lasso") {
    # Lasso特征选择
    x <- as.matrix(data[,-c(1:2)])
    y <- data$label
    cv_lasso <- cv.glmnet(x, y, family = "binomial", alpha = 1, nfolds = 10)
    coef_lasso <- coef(cv_lasso, s = "lambda.min")
    importance_scores <- abs(as.numeric(coef_lasso[-1]))
    names(importance_scores) <- features
    
  } else if(method == "boruta") {
    # Boruta特征选择
    boruta_result <- Boruta(label ~ ., data = data[,-1], doTrace = 0)
    confirmed_features <- names(boruta_result$finalDecision[boruta_result$finalDecision == "Confirmed"])
    return(confirmed_features)
  }
  
  # 排序并选择top特征
  sorted_features <- names(sort(importance_scores, decreasing = TRUE))
  
  if(is.null(top_n)) {
    # 保留重要性 > 0 的特征
    selected_features <- sorted_features[importance_scores[sorted_features] > 0]
  } else {
    selected_features <- head(sorted_features, top_n)
  }
  
  return(selected_features)
}

# 相关性特征过滤
correlation_filter <- function(data, threshold = 0.9) {
  cor_matrix <- cor(data[,-c(1:2)], use = "complete.obs")
  high_cor <- findCorrelation(cor_matrix, cutoff = threshold)
  
  if(length(high_cor) > 0) {
    remaining_features <- colnames(data)[-c(1:2)][-high_cor]
  } else {
    remaining_features <- colnames(data)[-c(1:2)]
  }
  
  return(remaining_features)
}

# 方差过滤
variance_filter <- function(data, threshold = 0.01) {
  variances <- apply(data[,-c(1:2)], 2, var, na.rm = TRUE)
  high_var_features <- names(variances[variances > threshold])
  return(high_var_features)
}
```

# 10种核心分类算法定义
# 10 Core Classification Algorithms Definition

```{r}
# 分类算法实现函数
classification_algorithms <- list(
  
  # 1. Random Forest
  "RF" = function(train_data, test_data, param = list()) {
    set.seed(seed)
    ntree <- ifelse(is.null(param$ntree), 500, param$ntree)
    mtry <- ifelse(is.null(param$mtry), sqrt(ncol(train_data)-2), param$mtry)
    
    model <- randomForest(as.factor(label) ~ ., 
                         data = train_data[,-1], 
                         ntree = ntree,
                         mtry = mtry,
                         importance = TRUE)
    
    pred_prob <- predict(model, test_data[,-c(1:2)], type = "prob")[,2]
    pred_class <- predict(model, test_data[,-c(1:2)])
    
    return(list(prob = pred_prob, class = pred_class, model = model))
  },
  
  # 2. Support Vector Machine (RBF kernel)
  "SVM" = function(train_data, test_data, param = list()) {
    set.seed(seed)
    cost <- ifelse(is.null(param$cost), 1, param$cost)
    gamma <- ifelse(is.null(param$gamma), 1/ncol(train_data), param$gamma)
    
    model <- svm(as.factor(label) ~ ., 
                data = train_data[,-1], 
                probability = TRUE, 
                kernel = "radial",
                cost = cost,
                gamma = gamma)
    
    pred_result <- predict(model, test_data[,-c(1:2)], probability = TRUE)
    pred_prob <- attr(pred_result, "probabilities")[,2]
    pred_class <- pred_result
    
    return(list(prob = pred_prob, class = pred_class, model = model))
  },
  
  # 3. Logistic Regression
  "LR" = function(train_data, test_data, param = list()) {
    tryCatch({
      model <- glm(label ~ ., 
                  data = train_data[,-1], 
                  family = "binomial")
      
      pred_prob <- predict(model, test_data[,-c(1:2)], type = "response")
      pred_class <- ifelse(pred_prob > 0.5, 1, 0)
      
      return(list(prob = pred_prob, class = pred_class, model = model))
    }, error = function(e) {
      # 如果模型拟合失败，返回随机预测
      n <- nrow(test_data)
      return(list(prob = rep(0.5, n), class = rep(0, n), model = NULL))
    })
  },
  
  # 4. Lasso Regression
  "Lasso" = function(train_data, test_data, param = list()) {
    set.seed(seed)
    alpha <- ifelse(is.null(param$alpha), 1, param$alpha)
    
    x_train <- as.matrix(train_data[,-c(1:2)])
    y_train <- train_data[,2]
    
    model <- cv.glmnet(x_train, y_train, 
                      family = "binomial", 
                      alpha = alpha, 
                      nfolds = 10)
    
    pred_prob <- predict(model, as.matrix(test_data[,-c(1:2)]), 
                        s = "lambda.min", type = "response")[,1]
    pred_class <- ifelse(pred_prob > 0.5, 1, 0)
    
    return(list(prob = pred_prob, class = pred_class, model = model))
  },
  
  # 5. Ridge Regression
  "Ridge" = function(train_data, test_data, param = list()) {
    set.seed(seed)
    alpha <- ifelse(is.null(param$alpha), 0, param$alpha)
    
    x_train <- as.matrix(train_data[,-c(1:2)])
    y_train <- train_data[,2]
    
    model <- cv.glmnet(x_train, y_train, 
                      family = "binomial", 
                      alpha = alpha, 
                      nfolds = 10)
    
    pred_prob <- predict(model, as.matrix(test_data[,-c(1:2)]), 
                        s = "lambda.min", type = "response")[,1]
    pred_class <- ifelse(pred_prob > 0.5, 1, 0)
    
    return(list(prob = pred_prob, class = pred_class, model = model))
  },
  
  # 6. Elastic Net
  "Enet" = function(train_data, test_data, param = list()) {
    set.seed(seed)
    alpha <- ifelse(is.null(param$alpha), 0.5, param$alpha)
    
    x_train <- as.matrix(train_data[,-c(1:2)])
    y_train <- train_data[,2]
    
    model <- cv.glmnet(x_train, y_train, 
                      family = "binomial", 
                      alpha = alpha, 
                      nfolds = 10)
    
    pred_prob <- predict(model, as.matrix(test_data[,-c(1:2)]), 
                        s = "lambda.min", type = "response")[,1]
    pred_class <- ifelse(pred_prob > 0.5, 1, 0)
    
    return(list(prob = pred_prob, class = pred_class, model = model))
  },
  
  # 7. XGBoost
  "XGBoost" = function(train_data, test_data, param = list()) {
    set.seed(seed)
    nrounds <- ifelse(is.null(param$nrounds), 100, param$nrounds)
    max_depth <- ifelse(is.null(param$max_depth), 6, param$max_depth)
    eta <- ifelse(is.null(param$eta), 0.3, param$eta)
    
    dtrain <- xgb.DMatrix(data = as.matrix(train_data[,-c(1:2)]), 
                         label = train_data[,2])
    dtest <- xgb.DMatrix(data = as.matrix(test_data[,-c(1:2)]))
    
    model <- xgboost(data = dtrain, 
                    nrounds = nrounds,
                    max_depth = max_depth,
                    eta = eta,
                    objective = "binary:logistic", 
                    verbose = 0)
    
    pred_prob <- predict(model, dtest)
    pred_class <- ifelse(pred_prob > 0.5, 1, 0)
    
    return(list(prob = pred_prob, class = pred_class, model = model))
  },
  
  # 8. Gradient Boosting Machine
  "GBM" = function(train_data, test_data, param = list()) {
    set.seed(seed)
    n.trees <- ifelse(is.null(param$n.trees), 500, param$n.trees)
    interaction.depth <- ifelse(is.null(param$interaction.depth), 3, param$interaction.depth)
    shrinkage <- ifelse(is.null(param$shrinkage), 0.01, param$shrinkage)
    
    model <- gbm(label ~ ., 
                data = train_data[,-1], 
                distribution = "bernoulli",
                n.trees = n.trees, 
                interaction.depth = interaction.depth,
                shrinkage = shrinkage,
                cv.folds = 5,
                verbose = FALSE)
    
    best_iter <- gbm.perf(model, method = "cv", plot.it = FALSE)
    pred_prob <- predict(model, test_data[,-c(1:2)], 
                        n.trees = best_iter, type = "response")
    pred_class <- ifelse(pred_prob > 0.5, 1, 0)
    
    return(list(prob = pred_prob, class = pred_class, model = model))
  },
  
  # 9. Neural Network
  "NN" = function(train_data, test_data, param = list()) {
    set.seed(seed)
    size <- ifelse(is.null(param$size), 5, param$size)
    decay <- ifelse(is.null(param$decay), 0.1, param$decay)
    
    tryCatch({
      model <- nnet(as.factor(label) ~ ., 
                   data = train_data[,-1], 
                   size = size,
                   decay = decay,
                   trace = FALSE,
                   maxit = 200)
      
      pred_prob <- predict(model, test_data[,-c(1:2)], type = "raw")[,1]
      pred_class <- ifelse(pred_prob > 0.5, 1, 0)
      
      return(list(prob = pred_prob, class = pred_class, model = model))
    }, error = function(e) {
      n <- nrow(test_data)
      return(list(prob = rep(0.5, n), class = rep(0, n), model = NULL))
    })
  },
  
  # 10. Naive Bayes
  "NB" = function(train_data, test_data, param = list()) {
    tryCatch({
      model <- naivebayes(as.factor(label) ~ ., 
                         data = train_data[,-1])
      
      pred_prob <- predict(model, test_data[,-c(1:2)], type = "prob")[,2]
      pred_class <- predict(model, test_data[,-c(1:2)])
      
      return(list(prob = pred_prob, class = pred_class, model = model))
    }, error = function(e) {
      n <- nrow(test_data)
      return(list(prob = rep(0.5, n), class = rep(0, n), model = NULL))
    })
  }
)
```

# 评估指标计算函数
# Evaluation Metrics Calculation Functions

```{r}
# 计算分类性能指标
calculate_metrics <- function(true_labels, pred_probs, pred_classes) {
  # AUC
  roc_obj <- roc(true_labels, pred_probs, quiet = TRUE)
  auc_val <- as.numeric(auc(roc_obj))
  
  # 混淆矩阵指标
  cm <- table(true_labels, pred_classes)
  
  if(nrow(cm) == 2 && ncol(cm) == 2) {
    tn <- cm[1,1]
    fp <- cm[1,2] 
    fn <- cm[2,1]
    tp <- cm[2,2]
    
    accuracy <- (tp + tn) / sum(cm)
    sensitivity <- tp / (tp + fn)  # 敏感性/召回率
    specificity <- tn / (tn + fp)  # 特异性
    precision <- tp / (tp + fp)    # 精确率
    f1_score <- 2 * (precision * sensitivity) / (precision + sensitivity)
    
    # 平衡准确率
    balanced_accuracy <- (sensitivity + specificity) / 2
    
  } else {
    # 如果分类结果有问题，设为默认值
    accuracy <- sensitivity <- specificity <- precision <- f1_score <- balanced_accuracy <- 0.5
  }
  
  return(list(
    AUC = auc_val,
    Accuracy = accuracy,
    Sensitivity = sensitivity,
    Specificity = specificity,
    Precision = precision,
    F1_Score = f1_score,
    Balanced_Accuracy = balanced_accuracy
  ))
}

# 批量评估函数
evaluate_model <- function(algorithm_name, train_data, test_data_list, param = list()) {
  results <- list()
  
  for(i in 1:length(test_data_list)) {
    test_data <- test_data_list[[i]]
    dataset_name <- names(test_data_list)[i]
    
    tryCatch({
      # 运行算法
      pred_result <- classification_algorithms[[algorithm_name]](train_data, test_data, param)
      
      # 计算指标
      metrics <- calculate_metrics(test_data$label, pred_result$prob, pred_result$class)
      
      # 存储结果
      result_row <- data.frame(
        Dataset = dataset_name,
        Model = algorithm_name,
        AUC = metrics$AUC,
        Accuracy = metrics$Accuracy,
        Sensitivity = metrics$Sensitivity,
        Specificity = metrics$Specificity,
        Precision = metrics$Precision,
        F1_Score = metrics$F1_Score,
        Balanced_Accuracy = metrics$Balanced_Accuracy,
        stringsAsFactors = FALSE
      )
      
      results[[dataset_name]] <- result_row
      
    }, error = function(e) {
      cat("Error in", algorithm_name, "for dataset", dataset_name, ":", e$message, "\n")
      
      # 错误情况下的默认结果
      result_row <- data.frame(
        Dataset = dataset_name,
        Model = algorithm_name,
        AUC = 0.5,
        Accuracy = 0.5,
        Sensitivity = 0.5,
        Specificity = 0.5,
        Precision = 0.5,
        F1_Score = 0.5,
        Balanced_Accuracy = 0.5,
        stringsAsFactors = FALSE
      )
      
      results[[dataset_name]] <- result_row
    })
  }
  
  return(do.call(rbind, results))
}
```

# 第一部分：单一算法评估
# Part 1: Single Algorithm Evaluation

```{r}
cat("=== 开始单一算法评估 ===\n")

# 存储所有结果
all_results <- data.frame()

# 算法名称
algo_names <- names(classification_algorithms)

# 1. 基础算法评估
for(algo in algo_names) {
  cat("正在运行算法:", algo, "\n")
  
  result <- evaluate_model(algo, est_data, val_data_list)
  all_results <- rbind(all_results, result)
}

cat("单一算法评估完成，共", nrow(all_results), "个结果\n\n")
```

# 第二部分：特征选择 + 算法组合
# Part 2: Feature Selection + Algorithm Combinations

```{r}
cat("=== 开始特征选择 + 算法组合 ===\n")

# 不同的特征选择方法
feature_selection_methods <- c("rf", "lasso", "boruta")

for(fs_method in feature_selection_methods) {
  cat("特征选择方法:", fs_method, "\n")
  
  # 进行特征选择
  if(fs_method == "boruta") {
    selected_features <- importance_feature_selection(est_data, method = fs_method)
  } else {
    selected_features <- importance_feature_selection(est_data, method = fs_method)
  }
  
  if(length(selected_features) == 0) {
    cat("特征选择失败，跳过\n")
    next
  }
  
  cat("选择了", length(selected_features), "个特征\n")
  
  # 准备特征选择后的数据
  est_data_fs <- est_data[, c(colnames(est_data)[1:2], selected_features)]
  val_data_list_fs <- lapply(val_data_list, function(x) {
    x[, c(colnames(x)[1:2], selected_features)]
  })
  
  # 对每个算法应用特征选择
  for(algo in algo_names) {
    cat("  运行:", fs_method, "+", algo, "\n")
    
    tryCatch({
      result <- evaluate_model(algo, est_data_fs, val_data_list_fs)
      result$Model <- paste0(fs_method, " + ", algo)
      all_results <- rbind(all_results, result)
    }, error = function(e) {
      cat("    错误:", e$message, "\n")
    })
  }
}

cat("特征选择 + 算法组合完成\n\n")
```

# 第三部分：Random Forest + 其他算法组合
# Part 3: Random Forest + Other Algorithm Combinations

```{r}
cat("=== 开始Random Forest特征选择 + 算法组合 ===\n")

# 使用Random Forest进行特征选择
set.seed(seed)
rf_model <- randomForest(as.factor(label) ~ ., 
                        data = est_data[,-1], 
                        ntree = 1000, 
                        importance = TRUE)

# 获取变量重要性
vimp_scores <- importance(rf_model)[,3]  # MeanDecreaseGini
selected_features_rf <- names(vimp_scores[vimp_scores > 0])

cat("RF特征选择：从", ncol(est_data)-2, "个特征中选择了", length(selected_features_rf), "个\n")

if(length(selected_features_rf) > 0) {
  # 准备RF特征选择后的数据
  est_data_rf <- est_data[, c(colnames(est_data)[1:2], selected_features_rf)]
  val_data_list_rf <- lapply(val_data_list, function(x) {
    x[, c(colnames(x)[1:2], selected_features_rf)]
  })
  
  # RF + Lasso组合
  cat("运行: RF + Lasso\n")
  result <- evaluate_model("Lasso", est_data_rf, val_data_list_rf)
  result$Model <- "RF + Lasso"
  all_results <- rbind(all_results, result)
  
  # RF + Ridge组合
  cat("运行: RF + Ridge\n")
  result <- evaluate_model("Ridge", est_data_rf, val_data_list_rf)
  result$Model <- "RF + Ridge"
  all_results <- rbind(all_results, result)
  
  # RF + Elastic Net组合（不同alpha值）
  for(alpha in seq(0.1, 0.9, 0.1)) {
    cat("运行: RF + Enet [α=", alpha, "]\n")
    
    param <- list(alpha = alpha)
    result <- evaluate_model("Enet", est_data_rf, val_data_list_rf, param)
    result$Model <- paste0("RF + Enet [α=", alpha, "]")
    all_results <- rbind(all_results, result)
  }
  
  # RF + XGBoost组合
  cat("运行: RF + XGBoost\n")
  result <- evaluate_model("XGBoost", est_data_rf, val_data_list_rf)
  result$Model <- "RF + XGBoost"
  all_results <- rbind(all_results, result)
  
  # RF + GBM组合
  cat("运行: RF + GBM\n")
  result <- evaluate_model("GBM", est_data_rf, val_data_list_rf)
  result$Model <- "RF + GBM"
  all_results <- rbind(all_results, result)
  
  # RF + SVM组合
  cat("运行: RF + SVM\n")
  result <- evaluate_model("SVM", est_data_rf, val_data_list_rf)
  result$Model <- "RF + SVM"
  all_results <- rbind(all_results, result)
  
  # RF + Neural Network组合
  cat("运行: RF + NN\n")
  result <- evaluate_model("NN", est_data_rf, val_data_list_rf)
  result$Model <- "RF + NN"
  all_results <- rbind(all_results, result)
  
  # RF + Naive Bayes组合
  cat("运行: RF + NB\n")
  result <- evaluate_model("NB", est_data_rf, val_data_list_rf)
  result$Model <- "RF + NB"
  all_results <- rbind(all_results, result)
}

cat("RF + 其他算法组合完成\n\n")
```

# 第四部分：Lasso特征选择 + 算法组合
# Part 4: Lasso Feature Selection + Algorithm Combinations

```{r}
cat("=== 开始Lasso特征选择 + 算法组合 ===\n")

# 使用Lasso进行特征选择
set.seed(seed)
x_train <- as.matrix(est_data[,-c(1:2)])
y_train <- est_data$label

cv_lasso <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1, nfolds = 10)
coef_lasso <- coef(cv_lasso, s = "lambda.min")
selected_features_lasso <- rownames(coef_lasso)[which(coef_lasso != 0)][-1]  # 排除截距

cat("Lasso特征选择：从", ncol(est_data)-2, "个特征中选择了", length(selected_features_lasso), "个\n")

if(length(selected_features_lasso) > 0) {
  # 准备Lasso特征选择后的数据
  est_data_lasso <- est_data[, c(colnames(est_data)[1:2], selected_features_lasso)]
  val_data_list_lasso <- lapply(val_data_list, function(x) {
    x[, c(colnames(x)[1:2], selected_features_lasso)]
  })
  
  # Lasso + Random Forest组合
  cat("运行: Lasso + RF\n")
  result <- evaluate_model("RF", est_data_lasso, val_data_list_lasso)
  result$Model <- "Lasso + RF"
  all_results <- rbind(all_results, result)
  
  # Lasso + XGBoost组合
  cat("运行: Lasso + XGBoost\n")
  result <- evaluate_model("XGBoost", est_data_lasso, val_data_list_lasso)
  result$Model <- "Lasso + XGBoost"
  all_results <- rbind(all_results, result)
  
  # Lasso + GBM组合
  cat("运行: Lasso + GBM\n")
  result <- evaluate_model("GBM", est_data_lasso, val_data_list_lasso)
  result$Model <- "Lasso + GBM"
  all_results <- rbind(all_results, result)
  
  # Lasso + SVM组合
  cat("运行: Lasso + SVM\n")
  result <- evaluate_model("SVM", est_data_lasso, val_data_list_lasso)
  result$Model <- "Lasso + SVM"
  all_results <- rbind(all_results, result)
  
  # Lasso + Logistic Regression组合
  cat("运行: Lasso + LR\n")
  result <- evaluate_model("LR", est_data_lasso, val_data_list_lasso)
  result$Model <- "Lasso + LR"
  all_results <- rbind(all_results, result)
  
  # Lasso + Neural Network组合
  cat("运行: Lasso + NN\n")
  result <- evaluate_model("NN", est_data_lasso, val_data_list_lasso)
  result$Model <- "Lasso + NN"
  all_results <- rbind(all_results, result)
}

cat("Lasso + 其他算法组合完成\n\n")
```

# 第五部分：集成学习组合
# Part 5: Ensemble Learning Combinations

```{r}
cat("=== 开始集成学习组合 ===\n")

# 投票集成函数
ensemble_voting <- function(algorithms, train_data, test_data_list, method = "average") {
  ensemble_results <- list()
  
  for(i in 1:length(test_data_list)) {
    test_data <- test_data_list[[i]]
    dataset_name <- names(test_data_list)[i]
    
    # 获取每个算法的预测概率
    pred_probs <- matrix(0, nrow = nrow(test_data), ncol = length(algorithms))
    
    for(j in 1:length(algorithms)) {
      algo <- algorithms[j]
      tryCatch({
        pred_result <- classification_algorithms[[algo]](train_data, test_data)
        pred_probs[, j] <- pred_result$prob
      }, error = function(e) {
        pred_probs[, j] <- rep(0.5, nrow(test_data))
      })
    }
    
    # 集成预测
    if(method == "average") {
      final_prob <- rowMeans(pred_probs)
    } else if(method == "median") {
      final_prob <- apply(pred_probs, 1, median)
    }
    
    final_class <- ifelse(final_prob > 0.5, 1, 0)
    
    # 计算指标
    metrics <- calculate_metrics(test_data$label, final_prob, final_class)
    
    result_row <- data.frame(
      Dataset = dataset_name,
      Model = paste(algorithms, collapse = " + "),
      AUC = metrics$AUC,
      Accuracy = metrics$Accuracy,
      Sensitivity = metrics$Sensitivity,
      Specificity = metrics$Specificity,
      Precision = metrics$Precision,
      F1_Score = metrics$F1_Score,
      Balanced_Accuracy = metrics$Balanced_Accuracy,
      stringsAsFactors = FALSE
    )
    
    ensemble_results[[dataset_name]] <- result_row
  }
  
  return(do.call(rbind, ensemble_results))
}

# 二元算法组合
binary_combinations <- list(
  c("RF", "XGBoost"),
  c("RF", "GBM"), 
  c("RF", "SVM"),
  c("Lasso", "Ridge"),
  c("Lasso", "XGBoost"),
  c("XGBoost", "GBM"),
  c("SVM", "LR"),
  c("RF", "LR"),
  c("Enet", "XGBoost"),
  c("GBM", "SVM")
)

for(combo in binary_combinations) {
  cat("运行集成:", paste(combo, collapse = " + "), "\n")
  
  tryCatch({
    result <- ensemble_voting(combo, est_data, val_data_list, method = "average")
    all_results <- rbind(all_results, result)
  }, error = function(e) {
    cat("  错误:", e$message, "\n")
  })
}

# 三元算法组合
ternary_combinations <- list(
  c("RF", "XGBoost", "SVM"),
  c("Lasso", "Ridge", "Enet"),
  c("RF", "GBM", "XGBoost"),
  c("SVM", "LR", "NB"),
  c("RF", "Lasso", "XGBoost")
)

for(combo in ternary_combinations) {
  cat("运行集成:", paste(combo, collapse = " + "), "\n")
  
  tryCatch({
    result <- ensemble_voting(combo, est_data, val_data_list, method = "average")
    all_results <- rbind(all_results, result)
  }, error = function(e) {
    cat("  错误:", e$message, "\n")
  })
}

cat("集成学习组合完成\n\n")
```

# 第六部分：参数调优组合
# Part 6: Parameter Tuning Combinations

```{r}
cat("=== 开始参数调优组合 ===\n")

# XGBoost参数调优
xgb_param <- list(
  list(nrounds = 50, max_depth = 3, eta = 0.1),
  list(nrounds = 100, max_depth = 6, eta = 0.3),
  list(nrounds = 200, max_depth = 4, eta = 0.2),
  list(nrounds = 150, max_depth = 5, eta = 0.25)
)

for(i in 1:length(xgb_param)) {
  param <- xgb_param[[i]]
  model_name <- paste0("XGBoost[n=", param$nrounds, ",d=", param$max_depth, ",η=", param$eta, "]")
  
  cat("运行:", model_name, "\n")
  
  tryCatch({
    result <- evaluate_model("XGBoost", est_data, val_data_list, param)
    result$Model <- model_name
    all_results <- rbind(all_results, result)
  }, error = function(e) {
    cat("  错误:", e$message, "\n")
  })
}

# Random Forest参数调优
rf_param <- list(
  list(ntree = 300, mtry = 3),
  list(ntree = 500, mtry = 5),
  list(ntree = 1000, mtry = 7),
  list(ntree = 800, mtry = 4)
)

for(i in 1:length(rf_param)) {
  param <- rf_param[[i]]
  model_name <- paste0("RF[ntree=", param$ntree, ",mtry=", param$mtry, "]")
  
  cat("运行:", model_name, "\n")
  
  tryCatch({
    result <- evaluate_model("RF", est_data, val_data_list, param)
    result$Model <- model_name
    all_results <- rbind(all_results, result)
  }, error = function(e) {
    cat("  错误:", e$message, "\n")
  })
}

# SVM参数调优
svm_param <- list(
  list(cost = 0.1, gamma = 0.001),
  list(cost = 1, gamma = 0.01),
  list(cost = 10, gamma = 0.1),
  list(cost = 100, gamma = 1)
)

for(i in 1:length(svm_param)) {
  param <- svm_param[[i]]
  model_name <- paste0("SVM[C=", param$cost, ",γ=", param$gamma, "]")
  
  cat("运行:", model_name, "\n")
  
  tryCatch({
    result <- evaluate_model("SVM", est_data, val_data_list, param)
    result$Model <- model_name
    all_results <- rbind(all_results, result)
  }, error = function(e) {
    cat("  错误:", e$message, "\n")
  })
}

cat("参数调优组合完成\n\n")
```

# 第七部分：结果分析和可视化
# Part 7: Results Analysis and Visualization

```{r}
cat("=== 开始结果分析 ===\n")

# 检查结果
cat("总共生成了", nrow(all_results), "个模型结果\n")

# 计算每个模型的平均性能
model_summary <- all_results %>%
  group_by(Model) %>%
  summarise(
    Mean_AUC = mean(AUC, na.rm = TRUE),
    SD_AUC = sd(AUC, na.rm = TRUE),
    Mean_Accuracy = mean(Accuracy, na.rm = TRUE),
    Mean_F1_Score = mean(F1_Score, na.rm = TRUE),
    Mean_Balanced_Accuracy = mean(Balanced_Accuracy, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  arrange(desc(Mean_AUC))

# 显示Top 20模型
cat("\nTop 20 模型 (按平均AUC排序):\n")
print(head(model_summary, 20))

# 保存详细结果
write.csv(all_results, "classification_detailed_results.csv", row.names = FALSE)
write.csv(model_summary, "classification_model_summary.csv", row.names = FALSE)
```

# 可视化结果
# Results Visualization

```{r fig.width=14, fig.height=10}
# 1. Top 20模型性能柱状图
top_20_models <- head(model_summary, 20)

p1 <- ggplot(top_20_models, aes(x = reorder(Model, Mean_AUC), y = Mean_AUC)) +
  geom_col(fill = "#4575B4", alpha = 0.8) +
  geom_errorbar(aes(ymin = pmax(0, Mean_AUC - SD_AUC), 
                    ymax = pmin(1, Mean_AUC + SD_AUC)), 
                width = 0.2, alpha = 0.7) +
  geom_text(aes(label = round(Mean_AUC, 3)), hjust = -0.1, size = 2.5) +
  coord_flip() +
  labs(title = "Top 20 Classification Models Performance",
       subtitle = "Error bars show ± 1 SD",
       x = "Model", y = "Mean AUC") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 12),
    axis.text.y = element_text(size = 8)
  )

print(p1)

# 2. 不同评估指标的比较
metrics_comparison <- top_20_models %>%
  select(Model, Mean_AUC, Mean_Accuracy, Mean_F1_Score, Mean_Balanced_Accuracy) %>%
  pivot_longer(cols = -Model, names_to = "Metric", values_to = "Value") %>%
  mutate(Metric = gsub("Mean_", "", Metric))

p2 <- ggplot(metrics_comparison, aes(x = reorder(Model, Value), y = Value, fill = Metric)) +
  geom_col(position = "dodge", alpha = 0.8) +
  coord_flip() +
  scale_fill_brewer(type = "qual", palette = "Set2") +
  labs(title = "Model Performance Across Different Metrics",
       x = "Model", y = "Performance Score") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    axis.text.y = element_text(size = 8),
    legend.position = "bottom"
  )

print(p2)
```

# 热图可视化
# Heatmap Visualization

```{r fig.width=12, fig.height=8}
# 准备热图数据
top_30_models <- head(model_summary, 30)
heatmap_data <- all_results %>%
  filter(Model %in% top_30_models$Model) %>%
  select(Dataset, Model, AUC) %>%
  pivot_wider(names_from = Dataset, values_from = AUC) %>%
  column_to_rownames('Model')

# 确保数据是数值型
heatmap_matrix <- as.matrix(heatmap_data)

# 绘制热图
library(ComplexHeatmap)
col_fun <- colorRamp2(c(0.5, 0.7, 0.85, 1), 
                     c("#2166AC", "#4393C3", "#FDBF6F", "#D73027"))

# 创建行注释
row_anno_data <- data.frame(
  Algorithm_Type = case_when(
    grepl("RF", rownames(heatmap_matrix)) ~ "Tree-based",
    grepl("SVM", rownames(heatmap_matrix)) ~ "Kernel",
    grepl("Lasso|Ridge|Enet|LR", rownames(heatmap_matrix)) ~ "Linear",
    grepl("XGBoost|GBM", rownames(heatmap_matrix)) ~ "Boosting",
    grepl("NN", rownames(heatmap_matrix)) ~ "Neural",
    grepl("NB", rownames(heatmap_matrix)) ~ "Probabilistic",
    TRUE ~ "Ensemble"
  )
)
rownames(row_anno_data) <- rownames(heatmap_matrix)

# 算法类型颜色
algo_colors <- c("Tree-based" = "#E31A1C", "Kernel" = "#1F78B4", 
                "Linear" = "#33A02C", "Boosting" = "#FF7F00",
                "Neural" = "#6A3D9A", "Probabilistic" = "#FB9A99",
                "Ensemble" = "#CAB2D6")

row_ha <- rowAnnotation(
  Algorithm_Type = row_anno_data$Algorithm_Type,
  col = list(Algorithm_Type = algo_colors),
  width = unit(0.5, "cm")
)

# 绘制热图
ht <- Heatmap(
  heatmap_matrix,
  name = "AUC",
  col = col_fun,
  rect_gp = gpar(col = "white", lwd = 1),
  show_row_names = TRUE,
  show_column_names = TRUE,
  row_names_side = "left",
  column_title = "Classification Performance Across Datasets",
  row_title = "Models (Top 30)",
  heatmap_legend_param = list(title = "AUC", title_position = "topleft"),
  row_names_gp = gpar(fontsize = 8),
  column_names_gp = gpar(fontsize = 10),
  left_annotation = row_ha,
  cluster_rows = TRUE,
  cluster_columns = FALSE
)

draw(ht, annotation_legend_side = "bottom")
```

# 最优模型详细分析
# Best Model Detailed Analysis

```{r fig.width=12, fig.height=8}
# 选择最优模型
best_model_name <- model_summary$Model[1]
best_model_results <- all_results %>% filter(Model == best_model_name)

cat("=== 最优模型详细分析 ===\n")
cat("最优模型:", best_model_name, "\n")
cat("平均AUC:", round(model_summary$Mean_AUC[1], 4), "\n")
cat("平均准确率:", round(model_summary$Mean_Accuracy[1], 4), "\n")
cat("平均F1分数:", round(model_summary$Mean_F1_Score[1], 4), "\n\n")

print(best_model_results)

# 模型性能分布箱线图
p3 <- all_results %>%
  filter(Model %in% head(model_summary$Model, 10)) %>%
  ggplot(aes(x = reorder(Model, AUC), y = AUC)) +
  geom_boxplot(fill = "#4575B4", alpha = 0.7) +
  geom_jitter(width = 0.2, alpha = 0.6, color = "#D73027") +
  coord_flip() +
  labs(title = "AUC Distribution for Top 10 Models",
       x = "Model", y = "AUC") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    axis.text.y = element_text(size = 9)
  )

print(p3)

# 算法类型性能比较
algo_type_performance <- all_results %>%
  mutate(
    Algorithm_Type = case_when(
      grepl("RF", Model) ~ "Tree-based",
      grepl("SVM", Model) ~ "Kernel",
      grepl("Lasso|Ridge|Enet|LR", Model) ~ "Linear",
      grepl("XGBoost|GBM", Model) ~ "Boosting",
      grepl("NN", Model) ~ "Neural",
      grepl("NB", Model) ~ "Probabilistic",
      grepl("\\+", Model) ~ "Ensemble",
      TRUE ~ "Other"
    )
  ) %>%
  group_by(Algorithm_Type) %>%
  summarise(
    Mean_AUC = mean(AUC, na.rm = TRUE),
    Median_AUC = median(AUC, na.rm = TRUE),
    Count = n(),
    .groups = 'drop'
  ) %>%
  arrange(desc(Mean_AUC))

p4 <- ggplot(algo_type_performance, aes(x = reorder(Algorithm_Type, Mean_AUC), y = Mean_AUC)) +
  geom_col(fill = "#33A02C", alpha = 0.8) +
  geom_text(aes(label = paste0(round(Mean_AUC, 3), "\n(n=", Count, ")")), 
            hjust = -0.1, size = 3) +
  coord_flip() +
  labs(title = "Algorithm Type Performance Comparison",
       x = "Algorithm Type", y = "Mean AUC") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"))

print(p4)
```

# 模型保存和导出
# Model Saving and Export

```{r}
cat("=== 保存结果和模型 ===\n")

# 保存最优模型信息
best_model_info <- list(
  best_model_name = best_model_name,
  best_model_performance = best_model_results,
  top_20_models = top_20_models,
  algorithm_type_performance = algo_type_performance,
  model_summary = model_summary,
  total_models_tested = nrow(model_summary)
)

saveRDS(best_model_info, "best_classification_model_info.rds")

# 保存绘图数据
plot_data <- list(
  heatmap_data = heatmap_data,
  metrics_comparison = metrics_comparison,
  algo_type_performance = algo_type_performance
)

saveRDS(plot_data, "classification_plot_data.rds")

# 生成模型报告
report <- list(
  summary = paste0("总共测试了 ", nrow(model_summary), " 种不同的分类模型组合"),
  best_model = paste0("最优模型: ", best_model_name, " (AUC: ", round(model_summary$Mean_AUC[1], 4), ")"),
  top_algorithm_type = paste0("最佳算法类型: ", algo_type_performance$Algorithm_Type[1], 
                             " (平均AUC: ", round(algo_type_performance$Mean_AUC[1], 4), ")"),
  dataset_count = length(val_data_list),
  feature_count = ncol(est_data) - 2
)

writeLines(unlist(report), "classification_model_report.txt")

cat("所有结果已保存:\n")
cat("- classification_detailed_results.csv: 详细结果\n")
cat("- classification_model_summary.csv: 模型总结\n")
cat("- best_classification_model_info.rds: 最优模型信息\n")
cat("- classification_plot_data.rds: 绘图数据\n")
cat("- classification_model_report.txt: 模型报告\n")

# 最终总结
cat("\n=== 分析完成总结 ===\n")
cat("最优模型:", best_model_name, "\n")
cat("最优AUC:", round(model_summary$Mean_AUC[1], 4), "\n")
cat("测试模型总数:", nrow(model_summary), "\n")
cat("使用数据集:", length(val_data_list) + 1, "个 (1训练 +", length(val_data_list), "验证)\n")
cat("输入特征数:", ncol(est_data) - 2, "\n")

# 关闭并行集群
stopCluster(cl)

cat("\n所有分析已完成！\n")
```

# Session Info

```{r}
sessionInfo()
```
