{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjzC8phjD4ii"
      },
      "source": [
        "# FigureYa104GEOmining\n",
        "\n",
        "title: \"FigureYa104GEOmining\"\n",
        "\n",
        "author: \"Yu Sun, Xuan Da, Taojun Ye\"\n",
        "\n",
        "reviewer: \"Ying Ge\"\n",
        "\n",
        "date: \"2025-5-20\"\n",
        "\n",
        "output: html_document\n",
        "\n",
        "## 需求描述\n",
        "\n",
        "在GEO数据库中检索到的高通量数据，想批量获得它们出自哪篇文章，标注影响因子，输出文本文件和网页，网页文件里有链接到每篇文章的pubmed页面。\n",
        "\n",
        "**用法参考这篇帖子：**<https://mp.weixin.qq.com/s/G-CQhNEJBmMRuDe2kxND_w>\n",
        "\n",
        "## Requirement Description\n",
        "\n",
        "Retrieve high-throughput data from the GEO database and obtain information on which articles they are from in bulk, annotate the impact factors, and output text files and web pages. The web page files should include links to the PubMed pages of each article. **For usage, refer to this post:** <https://mp.weixin.qq.com/s/G-CQhNEJBmMRuDe2kxND_w>\n",
        "\n",
        "## 应用场景\n",
        "\n",
        "场景一：老板让测序，怎样设计实验呢？参考测同样数据的文章是怎样设计实验的。\n",
        "\n",
        "场景二：测序数据回来了，怎样分析？能画哪些图？结果怎样描述？参考类似的数据的文章吧！\n",
        "\n",
        "场景三：想结合已发表的数据做整合分析，哪套数据更靠谱？先看影响因子高的文章里的数据吧！\n",
        "\n",
        "## Application Scenario\n",
        "\n",
        "Scenario 1: The boss asks for sequencing. How to design the experiment? Refer to how the experiment is designed in the article that measures the same data.\n",
        "\n",
        "Scenario 2: The sequencing data is back. How to analyze it? What kind of graphs can be drawn? How to describe the results? Refer to articles with similar data!\n",
        "\n",
        "Scenario 3: If you want to conduct a meta-analysis based on published data, which set of data is more reliable? Let's start with the data from articles with high impact factors!\n",
        "\n",
        "## 环境设置\n",
        "\n",
        "下载并安装Anaconda发行版，https://www.anaconda.com/distribution/#download-section\n",
        "\n",
        "里面已经包含了运行本文档所需的Python3、ipython、Jupyter notebook。\n",
        "\n",
        "需要额外安装BioPython、metapub和pytablewriter，由于eutils在比较新的版本中更新了API导致不能向前兼容，所以也需要重新安装一个比较旧的版本，在终端运行以下命令来安装：\n",
        "\n",
        "## Environment setup\n",
        "Download and install the Anaconda distribution from https://www.anaconda.com/distribution/#download-section\n",
        "It already includes Python 3, ipython, and Jupyter notebook required to run this document.\n",
        "Additional installations of BioPython, metapub, and pytablewriter are required. Since the API of eutils has been updated in newer versions, making them incompatible with previous versions, it is necessary to reinstall an older version. To install, run the following command in the terminal:\n",
        "\n",
        "```bash\n",
        "conda install -c anaconda biopython\n",
        "pip install metapub\n",
        "pip install eutils==0.5.0\n",
        "pip install pytablewriter\n",
        "pip install markdown\n",
        "pip install tqdm # Anaconda自带tqdm库# Anaconda comes with the tqdm library\n",
        "```\n",
        "\n",
        "## 输入\n",
        "\n",
        "把要检索的关键词写进下面代码区的”term = “的后面，例如`(gds pubmed[Filter]) AND \"Drosophila melanogaster\"[orgn:__txid7227] AND ATAC-seq`。\n",
        "\n",
        "建议先在<https://www.ncbi.nlm.nih.gov/geo/>网站上检索，尝试好合适的关键词后，再来提取文献。\n",
        "\n",
        "开头加上`(gds pubmed[Filter])` ，就会过滤掉那些还没有发表文章的数据。\n",
        "\n",
        "##Input\n",
        "Input the keyword you want to search into the code area below after \"term = \", for example, `(gds pubmed[Filter]) AND \"Drosophila melanogaster\"[orgn:__txid7227] AND ATAC-seq`.\n",
        "It is recommended to first search on the website <https://www.ncbi.nlm.nih.gov/geo/> and try using appropriate keywords before extracting relevant literature.\n",
        "Adding `(gds pubmed[Filter])` at the beginning will filter out data for those who have not yet published articles.\n",
        "\n",
        "## 运行代码\n",
        "\n",
        "打开Anaconda——Jupyter Notebook，打开FigureYa104GEOmining.ipynb文档，在Jupyter Notebook中点击Run按钮，运行下面的代码。\n",
        "\n",
        "**加速：**建议自己注册一个NCBI的账号，然后点击右上角自己的邮箱，申请API key，以加快检索速度：E-utils users are allowed 3 requests/second without an API key. Create an API key to increase your e-utils limit to 10 requests/second。\n",
        "\n",
        "把你的API key添加到代码区的“Entrez.api_key = ”后面\n",
        "\n",
        "API key的获取方法详见<https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/>\n",
        "\n",
        "## Running the code\n",
        "Open Anaconda - Jupyter Notebook, open the FigureYa104GEOmining.ipynb document, and click the \"Run\" button in Jupyter Notebook to run the code below.\n",
        "**Speeding up:** It is recommended to register for an NCBI account and then click on your email address in the top right corner to apply for an API key, in order to expedite the retrieval process: E-utils users are allowed 3 requests/second without an API key. Creating an API key will increase your e-utils limit to 10 requests/second.\n",
        "Add your API key after \"Entrez.api_key = \" in the code area\n",
        "For detailed information on how to obtain an API key, please refer to <https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/>\n",
        "\n",
        "## 下面是代码区\n",
        "\n",
        "## Below is the code area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcd8IB2fD4il"
      },
      "outputs": [],
      "source": [
        "from Bio import Entrez\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "from metapub import PubMedFetcher\n",
        "import re\n",
        "from pytablewriter import MarkdownTableWriter\n",
        "import sys\n",
        "from IPython.display import display, Markdown\n",
        "from markdown import markdown\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Python的默认迭代深度上限太低啦！基于BS4的爬虫在解析过程中很容易报错的\n",
        "# The default iteration depth limit of Python is too low! Crawlers based on BS4 are prone to errors during the parsing process\n",
        "sys.setrecursionlimit(999999)\n",
        "\n",
        "# 如果你还没申请自己的NCBI账号，就暂时先用作者的email\n",
        "# If you haven't applied for your own NCBI account, you can temporarily use the author's email\n",
        "Entrez.email = \"suny226@mail2.sysu.edu.cn\"\n",
        "# 如果申请了API key，就把它粘贴到 = 后面\n",
        "# If you have applied for an API key, paste it after the =\n",
        "#Entrez.api_key = \"yourAPIkey\"\n",
        "\n",
        "# 把你要检索的关键词写在这里\n",
        "# Write the keyword you want to search here\n",
        "#term = \"(gds pubmed[Filter]) AND Drosophila melanogaster[porgn:__txid7227] AND ATAC-seq\"\n",
        "#term = \"(gds pubmed[Filter]) AND Oryza sativa[porgn:__txid4530] AND ChIP-seq\"\n",
        "#term = \"(gds pubmed[Filter]) AND Arabidopsis thaliana[porgn:__txid3702] AND ChIP-seq\"\n",
        "term = \"(gds pubmed[Filter]) AND Homo sapiens[porgn:__txid9606] AND ChIP-seq AND YAP\"\n",
        "\n",
        "handle = Entrez.esearch(db=\"gds\",term=term, retmax = 100000)\n",
        "record = Entrez.read(handle)\n",
        "\n",
        "fetch = PubMedFetcher()\n",
        "journal_sep_regex = re.compile(r' |\\. ')\n",
        "\n",
        "writer = MarkdownTableWriter()\n",
        "writer.headers = [\"GEO accession\", \"PMID\", \"Journal\", \"2017/2018 Impact Factor\", \"Year\", \"Title\", \"Authors\"]\n",
        "writer.value_matrix = []\n",
        "\n",
        "print(\"The key words you searched with return {} results. Please wait parsing...\".format(len(record['IdList'])))\n",
        "\n",
        "with open('GEO_citations.txt', 'w') as f:\n",
        "    for id in tqdm(record['IdList']):\n",
        "        url = 'https://www.ncbi.nlm.nih.gov/gds/?term=' + id\n",
        "        soup = BeautifulSoup(urlopen(url), 'html.parser')\n",
        "        accn = soup.body.form('dl', class_=\"rprtid\")[0].contents[1].string\n",
        "        accn_url = 'https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=' + accn\n",
        "        soup = BeautifulSoup(urlopen(accn_url), 'html.parser')\n",
        "        pmids = soup.body.find('span', class_ = \"pubmed_id\")['id'].split(',')\n",
        "        for pmid in pmids:\n",
        "            pmid_url = 'https://www.ncbi.nlm.nih.gov/pubmed/' + pmid\n",
        "            article = fetch.article_by_pmid(pmid)\n",
        "\n",
        "            # 下面这个网址是letpub用来抓取影响因子的，应该是很靠谱了\n",
        "            # The following URL is used by letpub to retrieve impact factors, so it should be quite reliable\n",
        "            url = 'https://www.scijournal.org/impact-factor-of-' + journal_sep_regex.sub(\"-\", article.journal).upper().rstrip('.') + \".shtml\"\n",
        "            try:\n",
        "                soup = BeautifulSoup(urlopen(url), 'html.parser')\n",
        "                if_str = soup.body.find(text = re.compile('2017/2018 Impact Factor.+'))\n",
        "                if_value = if_str.replace('2017/2018 Impact Factor : ', \"\")\n",
        "                #该网站上有些期刊的2017/2018影响因子处为NA，可以把2017/2018改为2016\n",
        "                # Some journals on this website have their 2017/2018 impact factors marked as NA. You can replace 2017/2018 with 2016\n",
        "                #if_str = soup.body.find(text = re.compile('2016 Impact Factor.+'))\n",
        "                #if_value = if_str.replace('2016 Impact Factor : ', \"\")\n",
        "            except:\n",
        "                if_str = 'NA'\n",
        "                if_value = '-NA-'\n",
        "            author_list = \", \".join(article.authors[:-1]) + \" and \" + article.authors[-1]\n",
        "            f.write(\"{}: {}. {} {} {} {}\\n\".format(accn, author_list, article.title, article.journal, article.year, if_str))\n",
        "            writer.value_matrix.append(['[' + accn + '](' + accn_url + ')', '[' + pmid + '](' + pmid_url + ')', article.journal, if_value, article.year, article.title, author_list])\n",
        "\n",
        "with open('GEO_citations.html', 'w') as f:\n",
        "    table_html = markdown(writer.dumps(), extensions = ['markdown.extensions.tables'])\n",
        "    content = \"\"\"<html>\n",
        "<head></head>\n",
        "<body>{}</body>\n",
        "</html>\"\"\".format(table_html)\n",
        "    f.write(content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a81kh0vZD4im"
      },
      "source": [
        "## 输出\n",
        "\n",
        "在当前文件夹里生成分两个文件：\n",
        "\n",
        "1. `GEO_citations.txt`：文本文件，包含文章信息的汇总\n",
        "\n",
        "2. `GEO_citations.html`：网页中嵌入的三线表，蓝色字带链接，点击GSE ID可直达数据的GEO页面，点击PMID可直达文章的Pubmed页面。默认为按照相关性排序“Sort by Default order”，可以复制到Excel中自行排序、筛选等操作；链接也会保留到Excel文件中，点击链接可直接跳转至paper网页。\n",
        "\n",
        "##Output\n",
        "Generate two files in the current folder:\n",
        "1. `GEO_citations.txt`: a text file containing a summary of article information\n",
        "2. `GEO_citations.html`: The embedded three-line table in the webpage, with blue text and links. Clicking on the GSE ID will directly take you to the GEO page of the data, and clicking on the PMID will directly take you to the Pubmed page of the article. The default sorting order is \"Sort by Default order\", which can be copied to Excel for sorting, filtering, and other operations; the links will also be retained in the Excel file, and clicking on the link will directly take you to the paper webpage.\n",
        "\n",
        "## 特殊情况的说明\n",
        "\n",
        "1. 如果遇到TimeoutError，换个网络好的地方再试。\n",
        "\n",
        "2. 偶尔会遇到影响因子那里都是NA的情况，可能是<https://www.scijournal.org>网站访问不畅，稍后重试即可。\n",
        "\n",
        "3. 个别期刊在这个网站上检索不到影响因子，例如PNAS、NAR，所以会显示为NA。\n",
        "\n",
        "4. 如果文章题目中出现特殊符号，例如“<”，题目会在此断掉，这是Entrez包的一个bug。\n",
        "\n",
        "## Explanation of special circumstances\n",
        "1. If you encounter a TimeoutError, try again in a place with a better network connection.\n",
        "Occasionally, you may encounter a situation where all the impact factors are marked as NA. This may be due to poor access to the website <https://www.scijournal.org>. You can simply try again later.\n",
        "3. Some journals, such as PNAS and NAR, cannot be searched for their impact factors on this website, so they will be displayed as NA.\n",
        "4. If special symbols, such as \"<\", appear in the article title, the title will be broken here, which is a bug in the Entrez package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q67BdhgD4im"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "print(IPython.sys_info())\n",
        "\n",
        "!jupyter nbconvert --to html FigureYa104GEOmining.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCbNACvED4in"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}