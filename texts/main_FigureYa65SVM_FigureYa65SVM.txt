FigureYa65SVM
FigureYa65SVM
Author(s)
: Dongqiang Zeng, Yin Li; Ying Ge, Yijing
Chen
Date
: 2025-09-22
Academic Citation
If you use this code in your work or research, we kindly request that
you cite our publication:
Xiaofan Lu, et al. (2025). FigureYa: A Standardized Visualization
Framework for Enhancing Biomedical Data Interpretation and Research
Efficiency. iMetaMed.
https://doi.org/10.1002/imm3.70005
需求描述
Requirement description
利用svm-rfe和lasso logistic
regression进行特征变量筛选，重现原文方法
Feature variable screening using svm-rfe and lasso logistic
regression to reproduce the original method
出自
http://ascopubs.org/doi/full/10.1200/JCO.2016.68.2153
from
http://ascopubs.org/doi/full/10.1200/JCO.2016.68.2153
应用场景
Application scenario
svm-rfe（support vector machine - recursive feature
elimination）是基于支持向量机的机器学习方法，
通过删减svm产生的特征向量来寻找最佳变量。
lasso lr（logistic
regression）也是机器学习的方法之一，通过寻找分类错误最小时的λ来确定变量。
主要用于筛选特征变量，构建最佳分类模型。
svm-rfe (support vector machine - recursive feature elimination) is a
machine learning method based on support vector machines, which seeks
the optimal variables by eliminating the feature vectors generated by
svm.
lasso lr (logistic regression) is also one of the machine learning
methods that identifies variables by finding the λ with the smallest
classification error.
It is mainly used to filter the feature variables to construct the
best classification model.
环境设置
Environment setting
source("install_dependencies.R")
library(tidyverse)
library(glmnet)
source('msvmRFE.R')   #文件夹内自带 it comes with it in the folder
library(VennDiagram)
library(sigFeature)
library(e1071)
library(caret)
library(randomForest)
输入文件
Input file
easy_input.csv，带有分组信息的矩阵。此处用例文自带的GSE75041，经过处理后得到，处理过程见文件夹中的HowToGetEasyInput.R。
可自行准备，至少包含以下信息：
第一列：sample ID
第二列：样本分组信息，最好为二分类变量
第三列之后：表达矩阵
easy_input.csv, a matrix with grouping information. Here, the example
text’s own GSE75041 is used, obtained after processing, and the
processing procedure can be found in the HowToGetEasyInput.R file within
the folder.
It can be prepared by yourself and contains at least the following
information:
First column: sample ID
Second column: sample grouping information, preferably for
dichotomous variables
After the third column: the expression matrix
train <- read.csv("easy_input.csv",row.names = 1, 
                  as.is = F) #后面svmRFE函数要求group的类型为factor the following svmRFE function requires the type of group to be factor

dim(train)
train[1:4,1:4]
下面就以train作为输入，分别进行lasso和SVM-REF的变量筛选
Next, take train as the input to filter the variables of lasso and
SVM-REF respectively
图A，用LASSO-logitstic-Algorithm进行特征选择
Figure A, feature selection with LASSO-logitstic-Algorithm
找特征
Find features
# 转为lasso需要的格式
# convert to lasso required format
x <- as.matrix(train[,-1])  
(y <- ifelse(train$group == "NR", 0,1)) #把分组信息换成01 replace the grouping information with 01
#library(glmnet)
fit = glmnet(x, y, family = "binomial", alpha = 1, lambda = NULL)

# 画A图
# draw figure A
#pdf("A_lasso.pdf", width = 5, height = 5)
plot(fit, xvar = "dev", label = TRUE)
#dev.off()
注
：lasso图的更多细节调整，请参考FigureYa31lasso，例如在lasso图的线旁边显示特征名称，或用图例显示特征名称。
Note
: See FigureYa31lasso for more detailed
adjustments to the lasso plot, such as displaying feature names next to
the lines in the lasso plot, or displaying feature names with a
legend.
cvfit = cv.glmnet(x, y, 
                  nfold=10, #例文描述：10-fold cross-validation example text description: 10-fold cross-validation
                  family = "binomial", type.measure = "class")
plot(cvfit)
cvfit$lambda.min #查看最佳lambda view the best lambda
# 获取LASSO选出来的特征
# get the features selected by LASSO
myCoefs <- coef(cvfit, s="lambda.min");
lasso_fea <- myCoefs@Dimnames[[1]][which(myCoefs != 0 )]
(lasso_fea <- lasso_fea[-1])
# 把lasso找到的特征保存到文件
# save the features found by lasso to a file
write.csv(lasso_fea,"feature_lasso.csv")
效果评估
Effect evaluation
# 使用选出来的特征进行模型的预测
# 因为没有验证数据，这里用了训练数据
# use the selected features for model prediction
# since there is no validation data, training data is used here
predict <- predict(cvfit, newx = x[1:nrow(x),], s = "lambda.min", type = "class")
table(predict,y)
看到此处预测的准确性是百分之百，实际操作时还是要找测试集来做检验。
Seeing that the accuracy of the prediction here is 100%, it is still
necessary to find a test set for inspection during the actual
operation.
图B，使用SVM-REF-Algorithm-进行特征选择
Figure B, feature selection using SVM-REF-Algorithm-
例文用的是e1071包，参考资料：
johncolby的代码：
https://github.com/johncolby/SVM-RFE
http://www.colbyimaging.com/wiki/statistics/msvm-rfe
The example text uses the e1071 package, reference:
johncolby’s code:
https://github.com/johncolby/SVM-RFE
http://www.colbyimaging.com/wiki/statistics/msvm-rfe
找特征
Find features
#library(e1071)
#source(msvmRFE.R)
input <- train

#采用五折交叉验证 (k-fold crossValidation）
#use 5-fold cross validation (k-fold crossValidation）
svmRFE(input, k = 5, halve.above = 100) #分割数据，分配随机数 split data, assign random numbers
nfold = 5
nrows = nrow(input)
folds = rep(1:nfold, len=nrows)[sample(nrows)]
folds = lapply(1:nfold, function(x) which(folds == x)) 
results = lapply(folds, svmRFE.wrap, input, k=5, halve.above=100) #特征选择 feature selection
top.features = WriteFeatures(results, input, save=F) #查看主要变量 view main variables
head(top.features)
#把SVM-REF找到的特征保存到文件
#save the features found by SVM-REF to a file
write.csv(top.features,"feature_svm.csv")
效果评估（Estimate generalization error）
Effect evaluation (Estimate generalization error)
每个featsweep列表元素都对应于使用多个顶级特征（即 featsweep[1]
仅使用顶级特征，featsweep[2]
使用前2个特征，等等）。其中，svm.list包含外部5折交叉验证中每个10折的泛化误差估计值。这些精度的平均值为误差。
测试300个变量时运行了大概9个小时(16G-台式机)
Each featsweep list element corresponds to using that many of the top
features (i.e. featsweep[1] is using only the top feature, featsweep[2]
is using the top 2 features, etc.). Within each, svm.list contains the
generalization error estimates for each of the 10 folds in the external
5-fold CV. These accuracies are averaged as error.
Testing 300 variables took approximately 9 hours (16G-desktop)
# 运行时间主要取决于选择变量的个数，一般的电脑还是不要选择太多变量
# 选前5个变量进行SVM模型构建，体验一下
# the runtime depends on the number of variables selected, so don't select too many variables for a normal computer
# select the first 5 variables for SVM model construction, experience it
#featsweep = lapply(1:5, FeatSweep.wrap, results, input) #5个变量 5 variables
#featsweep

# 选前300个变量进行SVM模型构建，然后导入已经运行好的结果
# select the top 300 variables to construct the SVM model, and then import the already executed results
#featsweep = lapply(1:300, FeatSweep.wrap, results, input) #300个变量 300 variables
#save(featsweep,file = "featsweep.RData")
(load("featsweep.RData"))
# 画图
# draw plots
no.info = min(prop.table(table(input[,1])))
errors = sapply(featsweep, function(x) ifelse(is.null(x), NA, x$error))

#dev.new(width=4, height=4, bg='white')
#pdf("B_svm-error.pdf",width = 5,height = 5)
PlotErrors(errors, no.info=no.info) #查看错误率 view error rate
#dev.off()

#dev.new(width=4, height=4, bg='white')
#pdf("B_svm-accuracy.pdf",width = 5,height = 5)
Plotaccuracy(1-errors,no.info=no.info) #查看准确率 view accuracy rate
#dev.off()

# 图中红色圆圈所在的位置，即错误率最低点
# the position of the red circle in the figure, that is, the lowest point of the error rate
which.min(errors)
比较lasso和SVM-REF方法一找出的特征变量，画Venn图
Compare the feature variables found by lasso and SVM-REF method 1,
draw the Venn plot
(myoverlap <- intersect(lasso_fea, top.features[1:which.min(errors), "FeatureName"])) #交集 intersection
summary(lasso_fea%in%top.features[1:which.min(errors), "FeatureName"])
#pdf("C_lasso_SVM_venn.pdf", width = 5, height = 3)
grid.newpage()
venn.plot <- venn.diagram(list(LASSO = lasso_fea, #画图 draw a plot
                               SVM_RFE = as.character(top.features[1:which.min(errors),"FeatureName"])), NULL, 
                          fill = c("#E31A1C","#E7B800"), 
                          alpha = c(0.5,0.5), cex = 4, cat.fontface=3, 
                          category.names = c("LASSO", "SVM_RFE"), 
                          main = "Overlap")
grid.draw(venn.plot)
#dev.off()
跟例文的结果对比
Compare the results with the example
example_lasso.txt，例文用lasso找出来的特征
example_SVM-RFE.txt，例文用SVM-RFE找出来的特征
example_lasso.txt, features found by lasso in the example paper
example_SVM-RFE.txt, features found by SVM-RFE in the example
# lasso结果对比
# comparison of lasso result
ex_lasso <- read.table("example_lasso.txt")
dim(ex_lasso)
intersect(lasso_fea, ex_lasso$V1)
summary(lasso_fea%in%ex_lasso$V1)
# SVM结果对比
# comparison of SVM result
ex_SVM <- read.table("example_SVM-RFE.txt")
dim(ex_SVM)
intersect(top.features[1:which.min(errors), "FeatureName"], ex_SVM$V1)
summary(top.features[1:which.min(errors), "FeatureName"]%in%ex_SVM$V1)
# overlap对比
# overlap comparison
ex_overlap <- read.table("example_overlap.txt")
dim(ex_overlap)
intersect(myoverlap, ex_overlap$V1)
summary(myoverlap%in%ex_overlap$V1)
重合度非常低，小伙伴一起探讨吧！
There’s very little overlap, explore with your buddies!
附
Appendix
SVM-RFE算法有多套代码可以实现，这里提供了另外两种SVM-REF-Algorithm的实现方法。
实际操作中可分别运行，对比效果，然后选其一。
见Lasso-SVM-REF-algorithm-Feature
selection.R文件中的方法二和方法三。
There are several sets of codes to implement the SVM-REF algorithm,
and two other implementation methods of the SVM-REF-Algorithm are
provided here.
In practice, you can run them separately, compare the effect, and
then choose one of them.
See Methods 2 and Methods 3 in the Lasso-SVM-REF-algorithm-Feature
selection.R file.
Session Info
sessionInfo()